### KNN概述

- k最近邻,就是k个最近的邻居

- 一个样本在特征空间中的k个最近邻样本中大多数属于某一个类别/则该样本也属于这一个类别
- 所有的邻居都是已知类别的/而我们手上要进行分类的数据是不知道类别的



### 算法表述

- 计算测试数据和各个训练数据之间的距离
- 按照距离的递增关系进行排序
- 选择距离最小的k个点
- 确定前k个点所在类别的出现频率
- 返回前k个点中出现频率最高的类别作为测试数据的分类



### K的取值

- K:最邻近的每次计算点的个数
- K太小:容易被噪点影响/比如k=1的时候/恰好这个数据是个噪点/那么就完全影响了这次分类
- k太大:误差会增大/k最近邻做法的意义就变小了
- k尽量取奇数/方便预测



### 距离怎么计算的

- 欧几里得距离:$E(x,y)=\sqrt{\sum_{i=1}^{n}(x_{i}-y_{i})^{2}}$

- 曼哈顿距离:$D(x,y)=|x_{i}-x_{j}|+|y_{i}-y_{j}|$



### 总结

- knn是一个简单高效的分类算法并且容易实现
- 当训练集很大的时候/需要大量的存储空间/并且计算某个点和所有数据的距离是非常耗时的
- 对随机分布的数据分类效果差
- 对类内距离小/类间距离大的数据分类效果好
- 对样本不均匀的数据效果不好,可以进行权重改进,给近的测试点的权重更高
- KNN非常耗时,所以比较适合小规模数据