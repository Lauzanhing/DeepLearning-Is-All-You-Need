### 集成学习是什么

- 三种常见的集成学习框架:bagging/boosting和stacking



### bagging

- 从训练集中进行子集抽样组成每一个基础模型所需要的子训练集,对所有基础模型预测的结果进行综合产生最终的预测结果
- ![](https://imgkr.cn-bj.ufileos.com/298582cb-3b60-4e75-b621-d89a9e6b5a24.png)



### boosting

- 训练过程是阶梯型,基础模型按次序一一进行训练(实际上是可以并行的),基础模型的训练结果按照某种策略每次都进行一定的转化.对所有基础模型预测的结果进行线性综合产生最终的预测
- ![](https://imgkr.cn-bj.ufileos.com/394332a8-fd9f-45ab-9343-3de7c2d2c5c4.png)



### stacking

- 将训练好的所有基础模型对训练集进行预测,第j个基础模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值,最后基于新的训练集进行训练.
- 同理,预测的过程也要经过先经过所有基础模型预测形成的测试集,最后再对测试集进行预测
- ![](https://imgkr.cn-bj.ufileos.com/90db7069-f672-488c-8e91-bf4d66b2388c.png)



### 偏差和方差

- 偏差bias:预测值和真实值的差
- 方差variance:预测值作为随机变量的离散程度
- ![](https://imgkr.cn-bj.ufileos.com/2c6b6fd8-519f-4548-83dc-bd2547bdf9cf.png)
- Bagging和Stacking的基模型为强模型-偏差底,方差高
- Boosting的基模型是弱模型-偏差高,方差低



### 计算偏差和方差

- 假设基模型的期望为$\mu$,方差是$\sigma^{2}$,模型的权重为$r$,两个模型做的相关系数是$\rho$
- 对于bagging和boosting的基模型
- 总体期望

$$
\begin{align}   E(F) &= E(\sum_{i}^{m}{r_i f_i})   \\ &= \sum_{i}^{m}r_i E(f_i)   \end{align}  \\
$$

- 模型的总体方差

$$
\begin{align}   Var(F) &= Var(\sum_{i}^{m}{r_i f_i}) \\       &= \sum_{i}^{m}Var(r_if_i) + \sum_{i \neq j}^{m}Cov(r_i f_i ,  r_j f_j)   \\ &= \sum_{i}^{m} {r_i}^2 Var(f_i) + \sum_{i \neq j}^{m}\rho r_i r_j \sqrt{Var(f_i)} \sqrt{Var(f_j)} \\   &= mr^2\sigma^2 + m(m-1)\rho r^2 \sigma^2\\   &= m r^2 \sigma^2  (1-\rho) +  m^2 r^2 \sigma^2 \rho \end{align}  \\
$$

- 模型的精确度由偏差和方差共同决定

$$
Error = bias^2 + var + \xi
$$



### bagging的偏差和方差

- 对于每个基模型的权重等于$\frac{1}{m}$/所以公式可以写成

$$
\begin{align}   E(F) & = \sum_{i}^{m}r_i E(f_i)   \\      &= m \frac{1}{m} \mu \\     &= \mu  \\   Var(F) &=  m r^2 \sigma^2 (1-\rho) + m^2 r^2 \sigma^2 \rho \\     &= m \frac{1}{m^2} \sigma^2 (1-\rho) + m^2 \frac{1}{m^2} \sigma^2 \rho  \\     &= \frac{\sigma^2(1 - \rho)}{m}  + \sigma^2 \rho  \end{align}  \\
$$

- 总模型的期望等于基模型的期望等于基模型的期望/所以整体模型的偏差和基模型的偏差近似
- 总模型的方差小于等于基模型的方差,随着基模型的增多,整个模型的方差减少/泛化能力就强.精准度提高
- 所以Bagging的基模型一定要强模型,如果是用弱模型的话整体的偏差提高,准确度就降低了



### boosting的偏差和方差

- 因为boosting中,基模型公用训练集,也就是相关系数接近1

$$
\begin{align}  E(F) & = \sum_{i}^{m}r_i E(f_i) \\  Var(F) &= m r^2 \sigma^2  (1-\rho) +  m^2 r^2 \sigma^2 \rho \\   &= m \frac{1}{m^2} \sigma^2 (1-1) + m^2 \frac{1}{m^2} \sigma^2 1  \\&=  \sigma^2   \end{align}  \\
$$

- 总模型的方差等于基模型的方差/所以用弱模型/方差比较小/让他保持一定的泛化能力
- boosting采用贪心策略,总模型的期望由基模型的期望累加而曾,整体的准确度提高



### 总结

- bagging总模型偏差和基模型相似,随着模型增加可以降低整体模型的方差,所以基模型要强模型
- boosting总模型方差和基模型相似,随着模型增加可以降低整体模型的方差,所以基模型要弱模型
- 使用模型的偏差和方差来描述模型的准确度